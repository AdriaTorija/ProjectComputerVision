{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58729be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d9ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.rand([32, 65440])\n",
    "y = torch.rand([32,65440])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d03635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(p, y, gamma = 2, alpha=1):\n",
    "    \"\"\"\n",
    "    It used to suppress the presence of a large number of negative prediction.\n",
    "    It works on image level not batch level.\n",
    "    For any example/image, it keeps all the positive predictions and\n",
    "     cut the number of negative predictions to make sure the ratio\n",
    "     between the negative examples and positive examples is no more\n",
    "     the given ratio for an image.\n",
    "    Args:\n",
    "        loss (N, num_priors): the loss for each example.\n",
    "        labels (N, num_priors): the labels.\n",
    "        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n",
    "    \"\"\"\n",
    "    neg = 1 - p\n",
    "    term1 = torch.pow(neg,gamma)\n",
    "    term2 = y\n",
    "    term3 = torch.log(p)\n",
    "    \n",
    "    return -term1 * term2 * term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59845f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.2312e-07, 1.0796e-04, 1.1222e+00,  ..., 2.5963e+00, 2.0566e-02,\n",
       "         2.7834e-03],\n",
       "        [1.7884e-02, 8.8541e-02, 2.2278e-02,  ..., 2.7356e+00, 6.2279e-03,\n",
       "         1.2206e-05],\n",
       "        [1.6881e+00, 1.0304e-06, 7.1640e-02,  ..., 4.8990e-04, 1.1023e+00,\n",
       "         6.2865e-03],\n",
       "        ...,\n",
       "        [7.4628e-02, 6.2864e-11, 1.2448e+00,  ..., 5.6112e-05, 6.3003e-06,\n",
       "         2.3727e-01],\n",
       "        [9.0898e-02, 1.1591e-02, 3.2926e-01,  ..., 3.6088e-01, 1.4848e-03,\n",
       "         6.8981e-03],\n",
       "        [3.4742e-01, 1.3188e-01, 2.7378e-02,  ..., 9.4455e-02, 1.4322e-03,\n",
       "         2.3788e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal_loss(p,y, gamma=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(confs, y_pred, gamma=2.0):\n",
    "    #y_pred (tensor of shape (batch size, num_boxes, num_classes)\n",
    "    #confs: [32, 9, 65440]\n",
    "    alpha = torch.ones([len(confs[1])])\n",
    "    alpha[0] = 0.1\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, eps, 1.-eps)\n",
    "    pt = tf.where(tf.equal(y_true, 1), y_pred, 1- y_pred)\n",
    "    loss = -K.pow(1.-pt, gamma) * K.log(pt)\n",
    "    loss = alpha * loss\n",
    "    return tf.reduce_sum(loss, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2882d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'train.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py configs/task23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf26e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "my_list = [torch.randn(3, 5), torch.randn(3, 5)]\n",
    "result = torch.stack(my_list, dim=0).sum(dim=0)\n",
    "print(result.shape) #torch.Size([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357e14f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3585,  0.6455, -1.1953, -1.7481,  2.4313],\n",
       "        [ 0.6555, -0.7144,  2.0347, -1.1451,  1.3274],\n",
       "        [-1.2196,  0.5027,  0.4278, -0.0919,  0.1193]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eae21c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
