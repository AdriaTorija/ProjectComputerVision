{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280e80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "It looks like you are currently working on the server, with a 'working directory' in: /work/snotra/lautaror\n",
      "\tIf you struggle with NTNU home directory becoming full, we recommend you to change the output directory to: /work/snotra/lautaror\n",
      "\t /work/snotra/lautaror does not sync with NTNU HOME, and is a directory only located on the server.\n",
      "\t To change the output directory of SSD, set save_in_work to True in the file configs/utils.py, in the function get_output_dir.\n",
      "Saving SSD outputs to: outputs/\n",
      "Found dataset directory in: /work/datasets/mnist_object_detection/train\n",
      "Found dataset directory in: /work/datasets/mnist_object_detection/val\n",
      "Found dataset directory in: /work/datasets/tdt4265_2022\n",
      "Found dataset file in: /work/datasets/tdt4265_2022/train_annotations.json\n",
      "Found dataset directory in: /work/datasets/tdt4265_2022\n",
      "Found dataset file in: /work/datasets/tdt4265_2022/val_annotations.json\n",
      "--------------------Config file below--------------------\n",
      "{ 'anchors': { '_target_': <class 'ssd.modeling.anchor_boxes.AnchorBoxes'>,\n",
      "               'aspect_ratios': [[2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
      "               'feature_sizes': [ [32, 256],\n",
      "                                  [16, 128],\n",
      "                                  [8, 64],\n",
      "                                  [4, 32],\n",
      "                                  [2, 16],\n",
      "                                  [1, 8]],\n",
      "               'image_shape': '${train.imshape}',\n",
      "               'min_sizes': [ [16, 16],\n",
      "                              [32, 32],\n",
      "                              [48, 48],\n",
      "                              [64, 64],\n",
      "                              [86, 86],\n",
      "                              [128, 128],\n",
      "                              [128, 400]],\n",
      "               'scale_center_variance': 0.1,\n",
      "               'scale_size_variance': 0.2,\n",
      "               'strides': [ [4, 4],\n",
      "                            [8, 8],\n",
      "                            [16, 16],\n",
      "                            [32, 32],\n",
      "                            [64, 64],\n",
      "                            [128, 128]]},\n",
      "  'backbone': { '_target_': <class 'ssd.modeling.backbones.RetiNet.FPNResNets'>,\n",
      "                'image_channels': '${train.image_channels}',\n",
      "                'output_channels': [128, 256, 128, 128, 64, 64],\n",
      "                'output_feature_sizes': '${anchors.feature_sizes}'},\n",
      "  'data_train': { 'dataloader': { '_target_': <class 'torch.utils.data.dataloader.DataLoader'>,\n",
      "                                  'batch_size': '${...train.batch_size}',\n",
      "                                  'collate_fn': <function batch_collate at 0x7f1fc604e280>,\n",
      "                                  'dataset': '${..dataset}',\n",
      "                                  'drop_last': True,\n",
      "                                  'num_workers': 4,\n",
      "                                  'pin_memory': True,\n",
      "                                  'shuffle': True},\n",
      "                  'dataset': { '_target_': <class 'ssd.data.tdt4265.TDT4265Dataset'>,\n",
      "                               'annotation_file': '/work/datasets/tdt4265_2022/train_annotations.json',\n",
      "                               'img_folder': '/work/datasets/tdt4265_2022',\n",
      "                               'transform': '${train_cpu_transform}'},\n",
      "                  'gpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                     'transforms': [ { '_target_': <class 'ssd.data.transforms.gpu_transforms.Normalize'>,\n",
      "                                                       'mean': [ 0.4765,\n",
      "                                                                 0.4774,\n",
      "                                                                 0.2259],\n",
      "                                                       'std': [ 0.2951,\n",
      "                                                                0.2864,\n",
      "                                                                0.2878]}]}},\n",
      "  'data_val': { 'dataloader': { '_target_': <class 'torch.utils.data.dataloader.DataLoader'>,\n",
      "                                'batch_size': '${...train.batch_size}',\n",
      "                                'collate_fn': <function batch_collate_val at 0x7f1fc604e4c0>,\n",
      "                                'dataset': '${..dataset}',\n",
      "                                'num_workers': 4,\n",
      "                                'pin_memory': True,\n",
      "                                'shuffle': False},\n",
      "                'dataset': { '_target_': <class 'ssd.data.tdt4265.TDT4265Dataset'>,\n",
      "                             'annotation_file': '/work/datasets/tdt4265_2022/val_annotations.json',\n",
      "                             'img_folder': '/work/datasets/tdt4265_2022',\n",
      "                             'transform': '${val_cpu_transform}'},\n",
      "                'gpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                                   'transforms': [ { '_target_': <class 'ssd.data.transforms.gpu_transforms.Normalize'>,\n",
      "                                                     'mean': [ 0.4765,\n",
      "                                                               0.4774,\n",
      "                                                               0.2259],\n",
      "                                                     'std': [ 0.2951,\n",
      "                                                              0.2864,\n",
      "                                                              0.2878]}]}},\n",
      "  'gpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                     'transforms': [ { '_target_': <class 'ssd.data.transforms.gpu_transforms.Normalize'>,\n",
      "                                       'mean': [0.4765, 0.4774, 0.2259],\n",
      "                                       'std': [0.2951, 0.2864, 0.2878]}]},\n",
      "  'label_map': { 0: 'background',\n",
      "                 1: 'car',\n",
      "                 2: 'truck',\n",
      "                 3: 'bus',\n",
      "                 4: 'motorcycle',\n",
      "                 5: 'bicycle',\n",
      "                 6: 'scooter',\n",
      "                 7: 'person',\n",
      "                 8: 'rider'},\n",
      "  'loss_objective': { '_target_': <class 'ssd.modeling.focal_loss.SSDMultiboxLoss'>,\n",
      "                      'anchors': '${anchors}'},\n",
      "  'model': { '_target_': <class 'ssd.modeling.ssd.SSD300'>,\n",
      "             'anchors': '${anchors}',\n",
      "             'feature_extractor': '${backbone}',\n",
      "             'loss_objective': '${loss_objective}',\n",
      "             'num_classes': 9},\n",
      "  'optimizer': { '_target_': <class 'torch.optim.sgd.SGD'>,\n",
      "                 'lr': 0.005,\n",
      "                 'momentum': 0.9,\n",
      "                 'weight_decay': 0.0005},\n",
      "  'output_dir': PosixPath('outputs/task23'),\n",
      "  'run_name': '_task23',\n",
      "  'schedulers': { 'linear': { '_target_': <class 'torch.optim.lr_scheduler.LinearLR'>,\n",
      "                              'end_factor': 1,\n",
      "                              'start_factor': 0.1,\n",
      "                              'total_iters': 500},\n",
      "                  'multistep': { '_target_': <class 'torch.optim.lr_scheduler.MultiStepLR'>,\n",
      "                                 'gamma': 0.1,\n",
      "                                 'milestones': []}},\n",
      "  'train': { '_output_dir': PosixPath('outputs'),\n",
      "             'amp': True,\n",
      "             'batch_size': 32,\n",
      "             'epochs': 20,\n",
      "             'image_channels': 3,\n",
      "             'imshape': [128, 1024],\n",
      "             'log_interval': 20,\n",
      "             'seed': 0},\n",
      "  'train_cpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                           'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.RandomSampleCrop'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.gpu_transforms.ColorJitter'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.GaussianBlur'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.RandomAdjustSharpness'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.RandomHorizontalFlip'>},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                             'imshape': '${train.imshape}'},\n",
      "                                           { '_target_': <class 'ssd.data.transforms.target_transform.GroundTruthBoxesToAnchors'>,\n",
      "                                             'anchors': '${anchors}',\n",
      "                                             'iou_threshold': 0.5}]},\n",
      "  'val_cpu_transform': { '_target_': <class 'torchvision.transforms.transforms.Compose'>,\n",
      "                         'transforms': [ { '_target_': <class 'ssd.data.transforms.transform.ToTensor'>},\n",
      "                                         { '_target_': <class 'ssd.data.transforms.transform.Resize'>,\n",
      "                                           'imshape': '${train.imshape}'}]}}\n",
      "--------------------End of config file--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "loading annotations into memory...\n",
      "Done (t=0.11s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "SSD300                                Parameters  Buffers  Output shape       Datatype\n",
      "---                                   ---         ---      ---                ---     \n",
      "feature_extractor.backbone.conv1      9408        -        [1, 64, 64, 512]   float32 \n",
      "feature_extractor.backbone.bn1        128         129      [1, 64, 64, 512]   float32 \n",
      "feature_extractor.backbone.maxpool    -           -        [1, 64, 32, 256]   float32 \n",
      "feature_extractor.backbone.layer1     221952      774      [1, 64, 32, 256]   float32 \n",
      "feature_extractor.backbone.layer2     1116416     2313     [1, 128, 16, 128]  float32 \n",
      "feature_extractor.backbone.layer3     6822400     6669     [1, 256, 8, 64]    float32 \n",
      "feature_extractor.backbone.layer4     13114368    7175     [1, 512, 4, 32]    float32 \n",
      "feature_extractor.extra_layer.0       4719616     -        [1, 1024, 2, 16]   float32 \n",
      "feature_extractor.extra_layer.1       2048        2049     [1, 1024, 2, 16]   float32 \n",
      "feature_extractor.extra_layer2.0      9438208     -        [1, 1024, 1, 8]    float32 \n",
      "feature_extractor.extra_layer2.1      2048        2049     [1, 1024, 1, 8]    float32 \n",
      "feature_extractor.fpn.inner_blocks.4  262400      -        [1, 256, 1, 8]     float32 \n",
      "feature_extractor.fpn.layer_blocks.4  590080      -        [1, 256, 1, 8]     float32 \n",
      "feature_extractor.fpn.inner_blocks.4  -           -        [1, 256, 2, 16]    float32 \n",
      "feature_extractor.fpn.layer_blocks.4  -           -        [1, 256, 2, 16]    float32 \n",
      "feature_extractor.fpn.inner_blocks.3  131328      -        [1, 256, 4, 32]    float32 \n",
      "feature_extractor.fpn.layer_blocks.3  590080      -        [1, 256, 4, 32]    float32 \n",
      "feature_extractor.fpn.inner_blocks.2  65792       -        [1, 256, 8, 64]    float32 \n",
      "feature_extractor.fpn.layer_blocks.2  590080      -        [1, 256, 8, 64]    float32 \n",
      "feature_extractor.fpn.inner_blocks.1  33024       -        [1, 256, 16, 128]  float32 \n",
      "feature_extractor.fpn.layer_blocks.1  590080      -        [1, 256, 16, 128]  float32 \n",
      "feature_extractor.fpn.inner_blocks.0  16640       -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor.fpn.layer_blocks.0  590080      -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor:0                   513000      -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor:1                   -           -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor:2                   -           -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor:3                   -           -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor:4                   -           -        [1, 256, 32, 256]  float32 \n",
      "feature_extractor:5                   -           -        [1, 256, 32, 256]  float32 \n",
      "regression_heads.0                    55320       -        [1, 24, 32, 256]   float32 \n",
      "classification_heads.0                124470      -        [1, 54, 32, 256]   float32 \n",
      "regression_heads.1                    55320       -        [1, 24, 16, 128]   float32 \n",
      "classification_heads.1                124470      -        [1, 54, 16, 128]   float32 \n",
      "regression_heads.2                    55320       -        [1, 24, 8, 64]     float32 \n",
      "classification_heads.2                124470      -        [1, 54, 8, 64]     float32 \n",
      "regression_heads.3                    55320       -        [1, 24, 4, 32]     float32 \n",
      "classification_heads.3                124470      -        [1, 54, 4, 32]     float32 \n",
      "regression_heads.4                    36880       -        [1, 16, 2, 16]     float32 \n",
      "classification_heads.4                82980       -        [1, 36, 2, 16]     float32 \n",
      "regression_heads.5                    36880       -        [1, 16, 1, 8]      float32 \n",
      "classification_heads.5                82980       -        [1, 36, 1, 8]      float32 \n",
      "<top-level>:0                         261760      -        [1, 4, 65440]      float32 \n",
      "<top-level>:1                         -           -        [1, 4, 65440]      float32 \n",
      "---                                   ---         ---      ---                ---     \n",
      "Total                                 40639816    21158    -                  -       \n",
      "\n",
      "Epoch 0:   0%|                                           | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python train.py configs/task23.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815fb27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
